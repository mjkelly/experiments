# Downloads all http/https files referenced by a .har file. .har files are
# generated by browsers as a record of every network request they make during a
# session.
#
# I've only tested this script with .har files generated by Firefox.
#
# Files are organized by domain name. Any files that already exist in the
# output dir are not re-downloaded -- this way, you can recover from
# interruptions.
#
# There is a configurable cooldown interval to be nice to the remote server.
#
# Example usage:
#   python ./har-downloader.py myharfile.har mydir

from urllib.parse import urlparse
from urllib.request import urlopen
import json
import os
import os.path
import random
import shlex
import sys
import time

# This is the minimum amount of time to wait between downloads.
# We add up to 50% of jitter to this.
COOLDOWN_SECONDS = 2


def read_har(har_file):
    with open(har_file, 'r') as fh:
        data = json.load(fh)

    urls = []
    skipped = 0
    for entry in data['log']['entries']:
        url = entry['request']['url']

        if not url.startswith('http:') and not url.startswith('https:'):
            print(f'Skipping non-HTTP URL {url}')
            skipped += 1
        elif url.endswith('/'):
            print(f'Skipping directory {url}')
            skipped += 1
        else:
            urls.append(url)
    print(f'Found {len(urls)} viable URLs in {har_file}, skipped {skipped}')
    return urls


def download(url, destination):
    parsed = urlparse(url)
    dest_file = os.path.join(destination, parsed.netloc, parsed.path[1:])
    if parsed.query:
        dest_file = dest_file + '?' + parsed.query
    print(f"{url} -> {dest_file}")

    shlex.os.makedirs(os.path.dirname(dest_file), exist_ok=True)
    exists = False
    try:
        os.stat(dest_file)
        print(f"    SKIPPING: already exists")
        return False
    except FileNotFoundError:
        pass

    with urlopen(url) as response:
        with open(dest_file, 'wb') as fh:
            fh.write(response.read())
    return True


def main():
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} HAR_FILE DESTINATION_DIR")
        exit(2)
    print("===== READING HAR FILE =====")
    har_file, destination = sys.argv[1], sys.argv[2]
    urls = read_har(har_file)

    print("\n===== DOWNLOADING FILES =====")
    downloaded, existing = 0, 0
    try:
        for url in urls[5:]:
            if download(url, destination) and COOLDOWN_SECONDS != 0:
                downloaded += 1
                time.sleep(COOLDOWN_SECONDS +
                           (0.5 * COOLDOWN_SECONDS * random.random()))
            else:
                existing += 1
    except Exception as e:
        raise
    finally:
        print(f"Downloaded {downloaded}, {existing} files already existed.")


if __name__ == '__main__':
    main()
