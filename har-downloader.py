# Downloads all http/https files referenced by a .har file. .har files are
# generated by browsers as a record of every network request they make during a
# session.
#
# I've only tested this script with .har files generated by Firefox.
#
# Files are organized by domain name. Any files that already exist in the
# output dir are not re-downloaded -- this way, you can recover from
# interruptions.
#
# There is a configurable cooldown interval to be nice to the remote server.
#
# Example usage:
#   python ./har-downloader.py myharfile.har mydir

from urllib.parse import urlparse
from urllib.request import urlopen
import json
import os
import os.path
import random
import shlex
import sys
import time
import argparse


def read_har(args):
    with open(args.har_file, 'r') as fh:
        data = json.load(fh)

    urls = []
    skipped = 0
    for entry in data['log']['entries']:
        url = entry['request']['url']

        if not url.startswith('http:') and not url.startswith('https:'):
            print(f'Skipping non-HTTP URL {url}')
            skipped += 1
        elif url.endswith('/'):
            print(f'Skipping directory {url}')
            skipped += 1
        if args.url_pattern is None or args.url_pattern in url:
            urls.append(url)
        else:
            print(f'Skipping non-matcihng URL: {url}')
            skipped += 1
    print(
        f'Found {len(urls)} URLs to download from {args.har_file}. Skipped {skipped}.'
    )
    return urls


def download(url, destination, args):
    parsed = urlparse(url)
    dest_file = os.path.join(destination, parsed.netloc, parsed.path[1:])
    if args.save_query_string and parsed.query:
        dest_file = dest_file + '?' + parsed.query
    print(f"{url} -> {dest_file}")
    if args.dry_run:
        return False

    shlex.os.makedirs(os.path.dirname(dest_file), exist_ok=True)
    exists = False
    try:
        os.stat(dest_file)
        print(f"    SKIPPING: already exists")
        return False
    except FileNotFoundError:
        pass

    with urlopen(url) as response:
        with open(dest_file, 'wb') as fh:
            fh.write(response.read())
    return True


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("har_file")
    parser.add_argument("destination_dir")
    parser.add_argument(
        "--dry-run",
        help="Show what we would download and where it would go",
        action='store_true')
    parser.add_argument(
        "--save-query-string",
        help="If specified, save the full query string in the local filename. "
        "This can be useful if you have requests that differ only by query "
        "string - but it means you must find a way to serve these files "
        "separately on your own. (A static webserver won't do it.)",
        action='store_true')
    parser.add_argument(
        "--cooldown-seconds",
        help="This is the minimum amount of time to wait between downloads. "
        "We add up to 50%% of jitter to this.",
        type=int,
        default=2)
    parser.add_argument("--url-pattern",
                        help="Only download URLs containing this substring.")
    args = parser.parse_args()

    dry_run_str = "[DRY RUN] " if args.dry_run else ""
    print(f"===== {dry_run_str}READING HAR FILE =====")
    urls = read_har(args)

    print(f"\n===== {dry_run_str}DOWNLOADING FILES =====")
    downloaded, existing = 0, 0
    try:
        for url in urls[5:]:
            if download(url, args.destination_dir,
                        args) and args.cooldown_seconds != 0:
                downloaded += 1
                time.sleep(args.cooldown_seconds +
                           (0.5 * args.cooldown_seconds * random.random()))
            else:
                existing += 1
    except Exception as e:
        raise
    finally:
        print(
            f"{dry_run_str}Downloaded {downloaded}, {existing} files already existed."
        )


if __name__ == '__main__':
    main()
